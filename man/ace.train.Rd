% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/main_ace.R
\name{ace.train}
\alias{ace.train}
\title{Fit a VCM basis model with Gaussian process priors on the coefficients}
\usage{
ace.train(y, X, Z, kernel = "SE", basis = "linear", n.knots = 1,
  optimizer = "Nadam", maxiter = 1000, tol = 1e-04,
  learning_rate = 0.01, beta1 = 0.9, beta2 = 0.999, momentum = 0,
  norm.clip = TRUE, clip.at = 1)
}
\arguments{
\item{y}{A numeric vector}

\item{X}{A numeric vector or matrix}

\item{Z}{A vector or matrix (multivariate / tensor splines might be added in the future) Factors will be transformed to numeric using as.numeric. Hence, non-binary actors are discouraged especially when they are not ordinal.}

\item{kernel}{A string (default: "SE" Squared exponential with ARD) -- has no effect, might include (ARD) polynomial and "Matern32", the Matern 3/2 kernel with ARD}

\item{basis}{A string (default: "binary" if Z is binary (factor) and "ns", natural cubic spline, for continuous or discrete Z}

\item{n.knots}{An integer denoting the  umber of internal knots of the spline of Z. Ignored if basis not a spline.}

\item{optimizer}{A string (default: "Nadam" Nesterov-accelerated Adam). Other options are "GD" (gradient descent), "Nesterov" (accelerated gradient/momentum), "Adam".}

\item{maxiter}{(default: 1000) Maximum number of iterations of the empirical Bayes optimization}

\item{tol}{(default: 1e-4) Stopping tolerance for the empirical Bayes optimization}

\item{learning_rate}{(default: 0.001) Learning rate for the empirical Bayes optimization}

\item{beta1}{(default: 0.9) Learning parameter ("first moment") for the empirical Bayes optimization when using Adam or Nadam optimizers}

\item{beta2}{(default: 0.999) Learning parameter ("second moment") for the empirical Bayes optimization when using Adam or Nadam optimizers}

\item{momentum}{(default: 0.0) Momentum for the empirical Bayes optimization when using Nesterov. Equivalent to gradient descent ("GD") if momentum is 0.}
}
\value{
The function returns the fitted process as a GPspline class object. Predictions can be obtained using the corresponding S3 methods "prediction" and "marginal".
The latter is the predicted curve with a differentiated basis of Z.
}
\description{
Fit a VCM basis model with Gaussian process priors on the coefficients
}
\examples{
## Example replicating CausalStump with binary uni-variate Z
# Generate data
library(ace)
set.seed(1231)
n <- 120
Z <- rbinom(n, 1, 0.3)
X1 <- rnorm(sum(Z), mean = 30,sd = 10)
X0 <- rnorm(n-sum(Z), mean = 20, sd = 10)
X <- matrix(NaN, n, 1)
X[Z==1, ] <- X1
X[Z==0, ] <- X0
sort.idx <- sort(X, index.return = TRUE)$ix
y_truefun <- function(x,z) {
    mat0 <- matrix(72 + 3 * (x[z == 0,] > 0) * sqrt(abs(x[z == 0, ])), sum(z == 0), 1)
    mat1 <- matrix(90 + exp(0.06 * x[z == 1, ]), sum(z == 1), 1)
    mat <- matrix(NaN, length(z), 1)
    mat[z==0, 1] <- mat0
    mat[z==1, 1] <- mat1
    c(mat)
    }
y0_true <- y_truefun(X, rep(0, n))
y1_true <- y_truefun(X, rep(1, n))
Y0 <- rnorm(n, mean = y0_true, sd = 1)
Y1 <- rnorm(n, mean = y1_true, sd = 1)
Y <- Y0 * (1-Z) + Y1 * Z
# train model:
my.GPS <- ace.train(Y, X, Z, optimizer = "GD", learning_rate = 0.0001)
# print (sample) average treatment effect (ATE)
predict(my.GPS, marginal = TRUE, causal = TRUE)$ate_map
#true ATE
mean(y1_true - y0_true)
# plot response curves
plot(my.GPS, 1, marginal = FALSE, truefun = y_truefun)
# plot treatment curve
treat_truefun <- function(x) {y_truefun(x, rep(1, nrow(x))) - y_truefun(x, rep(0, nrow(x)))}
plot(my.GPS, 1, marginal = TRUE, truefun = treat_truefun)

## Example with continuous Z
set.seed(1234)
n2 <- 200
X2 <- matrix(runif(n2, min = 1, max = 2))
X3 <- matrix(runif(n2, min = 1, max = 2))
X <- data.frame(X2, X3)
Z2 <- rnorm(n2, exp(X2) - 10, 1)
y_truefun <- function(x, z) {as.matrix(10 * x + (x - 1.5) * ((z + 9)^2 - 2 * z))}
marg_truefun <- function(x, z) {as.matrix( (x - 1.5) * (2 * (z + 9) - 2))}
y2_true <- y_truefun(X2, Z2)
Y2 <- rnorm(n2, mean = y2_true, sd = 2)
marg_true <- marg_truefun(X2, Z2)
my.GPS <- ace.train(Y2, X2, Z2,
                    optimizer = "Nadam",
                    learning_rate = 0.01,
                    basis = "ncs")
plot(my.GPS, 1, truefun = y_truefun)
my.pred <- predict(my.GPS)
# plot quality of prediction:
plot(Y2, my.pred$map)
abline(0, 1, lty = 2, col = "blue", lwd = 2)
# comparison with the true curve:
plot(my.GPS, 1, truefun = y_truefun)
# plotting of the marginal curve:
plot(my.GPS, 1, marginal=TRUE)
# plot of the 2D curve with only Z
plot(my.GPS, truefun = y_truefun)

}
