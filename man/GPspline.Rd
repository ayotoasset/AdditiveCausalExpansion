% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/main_GPspline.R
\name{GPspline}
\alias{GPspline}
\title{Fir GPspline, data.frame wrapper of GPspline.train}
\usage{
GPspline(formula, data, kernel = "SE", spline = "ns", n.knots = 1,
  myoptim = "GD", maxiter = 1000, tol = 1e-04, learning_rate = 0.001,
  beta1 = 0.9, beta2 = 0.999, momentum = 0)
}
\arguments{
\item{formula}{Formula of the form \code{y ~ x | z} where the \code{x} variables determine the coefficients of the (spline) basis expansion of \code{z}.}

\item{data}{A data.frame containing the variables in the formula.}

\item{kernel}{A string (default: "SE" Squared exponential with ARD) -- has no effect, might include (ARD) polynomial and Matern 5/2 kernel}

\item{spline}{A string (default: "ns" natural cubic spline for continuous or discrete Z and "binary" if Z is binary (factor)}

\item{n.knots}{An integer denoting the  umber of internal knots of the spline of Z}

\item{myoptim}{A string (default: "GD" gradient descent). Other options are "Nesterov" (accelerated gradient/momentum), "Adam", and "Nadam" (Nesterov-Adam).}

\item{maxiter}{(default: 5000) Maximum number of iterations of the empirical Bayes optimization}

\item{tol}{(default: 1e-4) Stopping tolerance for the empirical Bayes optimization}

\item{learning_rate}{(default: 0.001) Learning rate for the empirical Bayes optimization}

\item{beta1}{(default: 0.9) Learning parameter ("first moment") for the empirical Bayes optimization when using Adam or Nadam optimizers}

\item{beta2}{(default: 0.999) Learning parameter ("second moment") for the empirical Bayes optimization when using Adam or Nadam optimizers}

\item{momentum}{(default: 0.0) Momentum for the empirical Bayes optimization when using Nesterov. Equivalent to gradient descent ("GD") if momentum is 0.}
}
\value{
GPspline object that can be used for prediction and plotting
}
\description{
Fir GPspline, data.frame wrapper of GPspline.train
}
\examples{
#continuous Z
set.seed(1234)
n2 <- 300
df <- data.frame(x = runif(n2, min = 1, max = 2))
df$x2 <- runif(n2, min = -1, max = 1)
df$z = rnorm(n2, exp(df$x)-14, 1)
y_truefun <- function(x,z) {as.matrix(sqrt(x[,1]) + x[,2] *3 * ((z+8)^2 - 2*z))}
y2_true <- y_truefun(df[,c("x","x2")],df$z)
df$y <- rnorm(n2, mean = y2_true, sd = 1)
my.GPS <- GPspline(y ~ x + x2 | z,data=df,myoptim="GD",learning_rate = 0.0001,spline="ns",n.knots=2)
my.pred <- predict(my.GPS)
plot(df$y,my.pred$map); abline(0,1,lty=2)
#prediction of the curve
plot(my.GPS,"x2",marginal=FALSE,plot3D=TRUE,plot.training=TRUE)
#difference to the true marginal curve:
marg_truefun <- function(x,z) {as.matrix(sqrt(x[,1]) + x[,2] *3 * (2*(z+8) - 2))}
plot(my.GPS,"x2",marginal=TRUE,plot.training=TRUE,truefun=marg_truefun)
}
